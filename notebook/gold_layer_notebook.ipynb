{"cells":[{"cell_type":"code","source":["from delta.tables import DeltaTable\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import *\n","import datetime"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"57db7538-4fb3-49f0-83ee-5538bd5f2ec7","normalized_state":"finished","queued_time":"2025-02-14T18:05:38.8298406Z","session_start_time":null,"execution_start_time":"2025-02-14T18:05:39.0793463Z","execution_finish_time":"2025-02-14T18:05:39.4457913Z","parent_msg_id":"c9979829-2e89-494f-ba44-76ddfb070382"},"text/plain":"StatementMeta(, 57db7538-4fb3-49f0-83ee-5538bd5f2ec7, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4dc48f8c-25c2-46e9-bfa4-d646f125a4bb"},{"cell_type":"code","source":["# Global configuration\n","UPDATED = datetime.datetime.today().replace(second=0, microsecond=0)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"57db7538-4fb3-49f0-83ee-5538bd5f2ec7","normalized_state":"finished","queued_time":"2025-02-14T18:05:39.2810447Z","session_start_time":null,"execution_start_time":"2025-02-14T18:05:39.6146137Z","execution_finish_time":"2025-02-14T18:05:40.0643268Z","parent_msg_id":"24f315ab-f089-4e92-8a52-00438ee1efce"},"text/plain":"StatementMeta(, 57db7538-4fb3-49f0-83ee-5538bd5f2ec7, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0c40ee05-00cb-43f7-9e49-0fd19bd79e57"},{"cell_type":"code","source":["# Paths and table names for Customers\n","CUSTOMER_GOLD_TABLE_PATH = \"Tables/dim_customer\"  # Gold table location for customers\n","CUSTOMER_SILVER_TABLE_NAME = \"customers\"           # Silver table name for customers\n","\n","# Define the schema for the Customers Gold table\n","customer_gold_schema = StructType([\n","    StructField(\"customer_id\", StringType(), True),\n","    StructField(\"customer_name\", StringType(), True),\n","    StructField(\"updated_at\", TimestampType(), True)\n","])\n","\n","\n","# Paths and table names for Products\n","PRODUCT_GOLD_TABLE_PATH = \"Tables/dim_product\"  # Gold table location for products\n","PRODUCT_SILVER_TABLE_NAME = \"products\"           # Silver table name for products\n","\n","# Define the schema for the Products Gold table\n","product_gold_schema = StructType([\n","    StructField(\"product_id\", StringType(), True),\n","    StructField(\"product_name\", StringType(), True),\n","    StructField(\"cost\", LongType(), True),\n","    StructField(\"original_sale_price\", LongType(), True),\n","    StructField(\"discount\", LongType(), True),\n","    StructField(\"current_price\", LongType(), True),\n","    StructField(\"taxes\", DoubleType(), True),\n","    StructField(\"updated_at\", TimestampType(), True)\n","])\n","\n","# Paths and table names for Locations\n","LOCATION_GOLD_TABLE_PATH = \"Tables/dim_location\"  # Gold table location for locations\n","LOCATION_SILVER_TABLE_NAME = \"locations\"           # Silver table name for locations\n","\n","# Define the schema for the Locations Gold table\n","location_gold_schema = StructType([\n","    StructField(\"location_id\", StringType(), True),\n","    StructField(\"name\", StringType(), True),\n","    StructField(\"county\", StringType(), True),\n","    StructField(\"state_code\", StringType(), True),\n","    StructField(\"state\", StringType(), True),\n","    StructField(\"type\", StringType(), True),\n","    StructField(\"latitude\", DoubleType(), True),\n","    StructField(\"longitude\", DoubleType(), True),\n","    StructField(\"updated_at\", TimestampType(), True)\n","])\n","\n","\n","def create_blank_df(spark, schema):\n","    \"\"\"Creates a blank DataFrame using the provided schema.\"\"\"\n","    return spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n","\n","\n","def process_dimension(spark, gold_table_path, silver_table_name, gold_schema, \n","                      select_exprs, key, additional_data, table_name):\n","    \"\"\"\n","    Processes a dimension table:\n","      - Reads the current watermark from the gold table (based on updated_at).\n","      - Creates the gold table if it doesn't exist.\n","      - Reads new records from the silver table (filtering by updated_at).\n","      - Selects the needed columns (using select_exprs).\n","      - Creates additional rows (using additional_data).\n","      - Unions the new records with additional rows.\n","      - Merges the unioned data into the gold Delta table.\n","      \n","    Parameters:\n","      spark: SparkSession.\n","      gold_table_path: Path to the gold table.\n","      silver_table_name: Silver table name in the metastore.\n","      gold_schema: Schema of the gold table.\n","      select_exprs: List of column expressions (or column names) to select from silver table.\n","      key: The key column used for the merge.\n","      additional_data: List of tuples containing additional row data (must match gold_schema).\n","      table_name: The name used when creating the table if it doesn't exist.\n","    \"\"\"\n","    try:\n","        gold_delta = DeltaTable.forPath(spark, gold_table_path)\n","        watermark_row = gold_delta.toDF().agg(F.max(\"updated_at\").alias(\"max_updated_at\")).collect()[0]\n","        watermark = watermark_row[\"max_updated_at\"]\n","        print(f\"[{table_name}] Current watermark: {watermark}\")\n","    except Exception as e:\n","        print(f\"[{table_name}] Gold table not found; initializing watermark. Exception: {e}\")\n","        watermark = datetime.datetime(1900, 1, 1)\n","        blank_df = create_blank_df(spark, gold_schema)\n","        blank_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n","        gold_delta = DeltaTable.forPath(spark, gold_table_path)\n","    \n","    df_silver = spark.table(silver_table_name)\n","    df_new = df_silver.filter(F.col(\"updated_at\") > F.lit(watermark))\n","    df_new_sel = df_new.select(*select_exprs)\n","    \n","    # Create additional rows DataFrame\n","    df_additional = spark.createDataFrame(additional_data, gold_schema)\n","    \n","    df_union = df_new_sel.unionByName(df_additional)\n","    new_count = df_union.count()\n","    print(f\"[{table_name}] New records to merge (including additional rows): {new_count}\")\n","    \n","    if new_count > 0:\n","        merge_condition = f\"tgt.{key} = src.{key}\"\n","        try:\n","            gold_delta.alias(\"tgt\").merge(\n","                df_union.alias(\"src\"),\n","                merge_condition\n","            ).whenMatchedUpdateAll() \\\n","             .whenNotMatchedInsertAll() \\\n","             .execute()\n","            print(f\"[{table_name}] Merge complete.\")\n","        except Exception as merge_error:\n","            print(f\"[{table_name}] Error during merge: {merge_error}\")\n","    else:\n","        print(f\"[{table_name}] No new records to merge.\")\n","\n","\n","process_dimension(\n","    spark,\n","    gold_table_path=CUSTOMER_GOLD_TABLE_PATH,\n","    silver_table_name=CUSTOMER_SILVER_TABLE_NAME,\n","    gold_schema=customer_gold_schema,\n","    select_exprs=[\"customer_id\", \"customer_name\", \"updated_at\"],\n","    key=\"customer_id\",\n","    additional_data=[(\"C0\", \"Undefined\", UPDATED), (\"C-1\", \"Invalid\", UPDATED)],\n","    table_name=\"dim_customer\"\n",")\n","\n","\n","process_dimension(\n","    spark,\n","    gold_table_path=PRODUCT_GOLD_TABLE_PATH,\n","    silver_table_name=PRODUCT_SILVER_TABLE_NAME,\n","    gold_schema=product_gold_schema,\n","    select_exprs=[\"product_id\", \"product_name\", \"cost\", \"original_sale_price\", \n","                  \"discount\", \"current_price\", \"taxes\", \"updated_at\"],\n","    key=\"product_id\",\n","    additional_data=[(\"P0\", \"Undefined\", 0, 0, 0, 0, 0.0, UPDATED),\n","                     (\"P-1\", \"Invalid\", 0, 0, 0, 0, 0.0, UPDATED)],\n","    table_name=\"dim_product\"\n",")\n","\n","\n","process_dimension(\n","    spark,\n","    gold_table_path=LOCATION_GOLD_TABLE_PATH,\n","    silver_table_name=LOCATION_SILVER_TABLE_NAME,\n","    gold_schema=location_gold_schema,\n","    select_exprs=[\"location_id\", \"name\", \"county\", \"state_code\", \"state\", \"type\",\n","                  \"latitude\", \"longitude\", \"updated_at\"],\n","    key=\"location_id\",\n","    additional_data=[(\"L0\", \"Undefined\", \"Undefined\", \"Undefined\", \"Undefined\", \"Undefined\", 0.0, 0.0, UPDATED),\n","                     (\"L-1\", \"Invalid\", \"Invalid\", \"Invalid\", \"Invalid\", \"Invalid\", 0.0, 0.0, UPDATED)],\n","    table_name=\"dim_location\"\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"57db7538-4fb3-49f0-83ee-5538bd5f2ec7","normalized_state":"finished","queued_time":"2025-02-14T18:05:40.0835089Z","session_start_time":null,"execution_start_time":"2025-02-14T18:05:40.2608637Z","execution_finish_time":"2025-02-14T18:06:26.7060014Z","parent_msg_id":"170d30d0-fb11-4ce1-a17e-b7fc3c394d4e"},"text/plain":"StatementMeta(, 57db7538-4fb3-49f0-83ee-5538bd5f2ec7, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[dim_customer] Gold table not found; initializing watermark. Exception: `Tables/dim_customer` is not a Delta table.\n[dim_customer] New records to merge (including additional rows): 803\n[dim_customer] Merge complete.\n[dim_product] Gold table not found; initializing watermark. Exception: `Tables/dim_product` is not a Delta table.\n[dim_product] New records to merge (including additional rows): 103\n[dim_product] Merge complete.\n[dim_location] Gold table not found; initializing watermark. Exception: `Tables/dim_location` is not a Delta table.\n[dim_location] New records to merge (including additional rows): 76\n[dim_location] Merge complete.\n"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b3e2e6af-1c60-4703-971d-f845328c4002"},{"cell_type":"code","source":["# Define start and end dates\n","start_date = \"2010-01-01\"\n","end_date = \"2030-12-31\"\n","\n","# Fiscal year start month (e.g., 7 for July)\n","_startOfFiscalYear = 7\n","\n","# Create a DataFrame with a sequence of dates between start_date and end_date\n","dim_date = spark.createDataFrame([(\"dummy\",)], [\"dummy\"]) \\\n","    .select(F.sequence(F.to_date(F.lit(start_date)), F.to_date(F.lit(end_date))).alias(\"DateArr\")) \\\n","    .selectExpr(\"explode(DateArr) as Date\")\n","\n","# Build the date dimension with additional columns\n","dim_date = dim_date.withColumn(\"Year\", F.year(\"Date\")) \\\n","    .withColumn(\"Start of Year\", F.expr(\"make_date(year(Date), 1, 1)\")) \\\n","    .withColumn(\"End of Year\", F.expr(\"make_date(year(Date), 12, 31)\")) \\\n","    .withColumn(\"Month\", F.month(\"Date\")) \\\n","    .withColumn(\"Start of Month\", F.expr(\"make_date(year(Date), month(Date), 1)\")) \\\n","    .withColumn(\"End of Month\", F.last_day(\"Date\")) \\\n","    .withColumn(\"Days in Month\", F.dayofmonth(F.last_day(\"Date\"))) \\\n","    .withColumn(\"Year Month Number\", F.expr(\"cast(date_format(Date, 'yyyyMM') as int)\")) \\\n","    .withColumn(\"Year Month Name\", F.date_format(\"Date\", \"yyyy-MMM\")) \\\n","    .withColumn(\"Day\", F.dayofmonth(\"Date\")) \\\n","    .withColumn(\"Day Name\", F.date_format(\"Date\", \"EEEE\")) \\\n","    .withColumn(\"Day Name Short\", F.date_format(\"Date\", \"EEE\")) \\\n","    .withColumn(\"Day of Week\", F.dayofweek(\"Date\")) \\\n","    .withColumn(\"Day of Year\", F.dayofyear(\"Date\")) \\\n","    .withColumn(\"Month Name\", F.date_format(\"Date\", \"MMMM\")) \\\n","    .withColumn(\"Month Name Short\", F.date_format(\"Date\", \"MMM\")) \\\n","    .withColumn(\"Quarter\", F.quarter(\"Date\")) \\\n","    .withColumn(\"Quarter Name\", F.concat(F.lit(\"Q\"), F.quarter(\"Date\"))) \\\n","    .withColumn(\"Year Quarter Number\", F.expr(\"cast(date_format(Date, 'yyyy') || date_format(Date, 'Q') as int)\")) \\\n","    .withColumn(\"Year Quarter Name\", F.concat(F.year(\"Date\"), F.lit(\" Q\"), F.quarter(\"Date\"))) \\\n","    .withColumn(\"Start of Quarter\", F.expr(\"make_date(year(Date), ((quarter(Date)-1)*3)+1, 1)\")) \\\n","    .withColumn(\"End of Quarter\", F.expr(\"date_sub(add_months(make_date(year(Date), ((quarter(Date)-1)*3)+1, 1), 3), 1)\")) \\\n","    .withColumn(\"Week of Year\", F.weekofyear(\"Date\")) \\\n","    .withColumn(\"Start of Week\", F.expr(\"date_sub(Date, dayofweek(Date)-1)\")) \\\n","    .withColumn(\"End of Week\", F.expr(\"date_add(date_sub(Date, dayofweek(Date)-1), 6)\")) \\\n","    .withColumn(\"Fiscal Year\", \n","                F.expr(f\"case when month(Date) >= {_startOfFiscalYear} then year(Date)+1 else year(Date) end\")) \\\n","    .withColumn(\"Fiscal Month\", (((F.month(\"Date\") - _startOfFiscalYear + 12) % 12) + 1)) \\\n","    .withColumn(\"Fiscal Quarter\", \n","                F.expr(f\"ceil((((month(Date) - {_startOfFiscalYear} + 12) % 12) + 1)/3.0)\")) \\\n","    .withColumn(\"Day Offset\", F.datediff(F.current_date(), \"Date\")) \\\n","    .withColumn(\"Month Offset\", F.months_between(F.current_date(), \"Date\").cast(\"int\")) \\\n","    .withColumn(\"Quarter Offset\", (F.months_between(F.current_date(), \"Date\")/3).cast(\"int\")) \\\n","    .withColumn(\"Year Offset\", F.year(F.current_date()) - F.year(\"Date\")) \\\n","    .withColumn(\"Year-month\", F.date_format(\"Date\", \"yyyy-MM\"))\n","\n","# Write the dim_date DataFrame as a Delta table with column mapping enabled.\n","dim_date.write.format(\"delta\") \\\n","    .option(\"delta.columnMapping.mode\", \"name\") \\\n","    .option(\"delta.minReaderVersion\", \"2\") \\\n","    .option(\"delta.minWriterVersion\", \"5\") \\\n","    .mode(\"overwrite\") \\\n","    .saveAsTable(\"dim_date\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"57db7538-4fb3-49f0-83ee-5538bd5f2ec7","normalized_state":"finished","queued_time":"2025-02-14T18:10:16.2831001Z","session_start_time":null,"execution_start_time":"2025-02-14T18:10:16.4903576Z","execution_finish_time":"2025-02-14T18:10:36.5033843Z","parent_msg_id":"19ca9c1c-9327-45f2-b616-1eb6408bee3f"},"text/plain":"StatementMeta(, 57db7538-4fb3-49f0-83ee-5538bd5f2ec7, 13, Finished, Available, Finished)"},"metadata":{}}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"72188c53-8916-43ae-857d-ee14598e3f0b"},{"cell_type":"code","source":["FACT_ORDER_GOLD_TABLE_PATH = \"Tables/fact_order\"        # Gold table location for fact_order\n","FACT_ORDER_SILVER_TABLE_NAME = \"sales\"                   # Silver table name for sales data\n","\n","# Define the schema for the fact_order table.\n","# Note: An extra \"updated_at\" field is added for incremental loading.\n","fact_order_schema = StructType([\n","    StructField(\"order_id\", StringType(), True),\n","    StructField(\"product_id\", StringType(), True),\n","    StructField(\"location_id\", StringType(), True),\n","    StructField(\"customer_id\", StringType(), True),\n","    StructField(\"order_date\", DateType(), True),\n","    StructField(\"quantity\", LongType(), True),\n","    StructField(\"price\", LongType(), True),\n","    StructField(\"updated_at\", TimestampType(), True)\n","])\n","\n","# 1. Determine the current watermark from the fact_order (Gold) table.\n","try:\n","    fact_order_delta = DeltaTable.forPath(spark, FACT_ORDER_GOLD_TABLE_PATH)\n","    watermark_row = fact_order_delta.toDF().agg(F.max(\"updated_at\").alias(\"max_updated_at\")).collect()[0]\n","    watermark = watermark_row[\"max_updated_at\"]\n","    print(f\"[Fact Order] Current watermark: {watermark}\")\n","except Exception as e:\n","    print(f\"[Fact Order] Table not found; initializing watermark. Exception: {e}\")\n","    watermark = datetime.datetime(1900, 1, 1)\n","    blank_df = create_blank_df(spark, fact_order_schema)\n","    blank_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"fact_order\")\n","    fact_order_delta = DeltaTable.forPath(spark, FACT_ORDER_GOLD_TABLE_PATH)\n","\n","# 2. Read only new records from the Silver sales table using the watermark.\n","df_silver = spark.table(FACT_ORDER_SILVER_TABLE_NAME)\n","# Assume the silver table has an \"updated_date\" column for incremental processing.\n","df_new = df_silver.filter(F.col(\"updated_date\") > F.lit(watermark))\n","\n","# 3. Coalesce key columns to provide default IDs if null.\n","#    Also, move \"updated_date\" to \"updated_at\" for our merge and incremental load.\n","df_new = df_new.withColumn(\"product_id\", F.coalesce(F.col(\"product_id\"), F.lit(\"P0\"))) \\\n","               .withColumn(\"location_id\", F.coalesce(F.col(\"location_id\"), F.lit(\"L0\"))) \\\n","               .withColumn(\"customer_id\", F.coalesce(F.col(\"customer_id\"), F.lit(\"C0\"))) \\\n","               .withColumn(\"updated_at\", F.col(\"updated_date\"))\n","\n","# 4. Select the necessary columns in the order defined by our schema.\n","df_new_sel = df_new.select(\"order_id\", \"product_id\", \"location_id\", \"customer_id\",\n","                           \"order_date\", \"quantity\", \"price\", \"updated_at\")\n","\n","# Check if there are new records to merge.\n","new_count = df_new_sel.count()\n","print(f\"[Fact Order] New records to merge: {new_count}\")\n","\n","# 5. Merge (upsert) the new records into the fact_order table.\n","if new_count > 0:\n","    merge_condition = \"tgt.order_id = src.order_id\"\n","    try:\n","        fact_order_delta.alias(\"tgt\").merge(\n","            df_new_sel.alias(\"src\"),\n","            merge_condition\n","        ).whenMatchedUpdateAll() \\\n","         .whenNotMatchedInsertAll() \\\n","         .execute()\n","        print(\"[Fact Order] Merge complete.\")\n","    except Exception as merge_error:\n","        print(f\"[Fact Order] Error during merge: {merge_error}\")\n","else:\n","    print(\"[Fact Order] No new records to merge.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"57db7538-4fb3-49f0-83ee-5538bd5f2ec7","normalized_state":"finished","queued_time":"2025-02-14T18:10:51.9810175Z","session_start_time":null,"execution_start_time":"2025-02-14T18:10:52.2252532Z","execution_finish_time":"2025-02-14T18:11:05.5535457Z","parent_msg_id":"89738ff0-a80b-47c7-b707-3bda0c8d0dab"},"text/plain":"StatementMeta(, 57db7538-4fb3-49f0-83ee-5538bd5f2ec7, 14, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[Fact Order] Table not found; initializing watermark. Exception: `Tables/fact_order` is not a Delta table.\n[Fact Order] New records to merge: 10889\n[Fact Order] Merge complete.\n"]}],"execution_count":12,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3c965d0b-e036-4ab5-9d19-822e7320fa25"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"0892e8da-f458-4f07-9c3b-2e149fa4fa55","default_lakehouse_name":"lakehouse","default_lakehouse_workspace_id":"6d576874-e554-484d-8cca-48813fbc4973"}}},"nbformat":4,"nbformat_minor":5}